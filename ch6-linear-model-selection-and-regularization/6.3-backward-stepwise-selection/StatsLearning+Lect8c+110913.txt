So that's forward stepwise selection.
And now we're briefly going to talk about backward stepwise.
And backward stepwise is, once again, just like forward.
It's an efficient alternative to best subset selection.
But it actually is exactly the opposite of forward stepwise.
So remember, in forward stepwise, we built the model
m0 and then we added a feature to get m1, added a
feature to get m2.
And in contrast, for backward stepwise, we're going
to start with mp--
which is the model containing p predictors--
and we're going to remove predictors one at time, one at
a time, until we get to m0, which is the model with just
an intercept.
So in a little more detail, we start with mp, which is just
the regular least squares model if you just didn't do
any feature selection and you just wanted to use all of your
predictors.
And then we're going to take that model mp and we're going
to consider removing predictors one at a time.
Excuse me, we're going to consider removing each of the
p predictors.
And we're going to see which predictor is the
least useful to.
Which predictor can we remove that's going to have to the
smallest effect on either RSS or R squared?
So we're going to remove the least useful predictor.
And that'll give us the model mp minus 1.
And then we're going to take mp minus 1, and we're going to
again ask which predictor is the least useful?
Which one can we remove in order to have the least
detrimental effect on our model fit?
And we're going to keep doing this until we make it down to
the model with just one predictor, m1, and finally to
the model which was no predictors, which is m0.
So that's once again going to give us a set of
models from m0 to mp.
And we're going to choose among them using, once again,
cross validation, AIC, BIC, or adjusted r squared.
So just as was the case for forward stepwise selection,
backward stepwise considers around p squared models
instead of 2 to the p.
So it's a fantastic computational alternative to
best subset selection, especially when p
is moderate or large.
And actually, the exact number of models that backward
selection considers, you know I've been saying
it's around p squared.
It's actually more like p squared over 2.
That's the exact formula.
And that's a good thing to work out at home.
So think about it.
Why is it that if you do forward stepwise selection or
backward stepwise selection, you're actually considering
this number of models?
So just like forward stepwise selection, backward stepwise
is not guaranteed to give us the best model containing a
particular subset of the p predictors.
So it was that same picture that Rob drew a few minutes
ago, where we thought that forward stepwise for any given
model size might give a larger RSS--
or equivalently, a smaller r squared--
than best subset.
So similarly, backward stepwise may not give quite as
good of a model in terms of RSS or r squared.
But that can still be OK.
And in the long run, it could give us better results on a
test set, not to mention it's more
computationally efficient.
So one major difference as we mentioned between backward and
forward stepwise is that with backward stepwise we start
with the model containing all of the predictors.
So in order to be able to do that, we need to be in a
situation where we have more observations than variables.
Because remember, we can do least squares regression when
n is greater than p.
But if p is greater than n, we cannot fit a
least squares model.
It's not even defined.
So we can only do backward selection when n is
greater than p.
And in contrast, we can do forward stepwise whether n is
less than p or n is greater than p.
So we mentioned that the model containing all of the
predictors is always going to have a smaller RSS and a
larger R squared than any other model, because these
quantities are really related to the training error.
So again, this is the picture that Rob drew.
But if I think about fitting a model with the number of
predictors on the y-axis and RSS on the--
if I think about fitting a model with the number of
predictors on the x-axis and RSS on the y-axis, this is
going to be monotone decreasing.
And similarly, if I think about a picture with number
predictor is on the x-axis and R squared on the y-axis, it's
going to be monotone increasing.
So remember, when we do best subset or forward or backward
stepwise, we end up with these models m0 through mp.
And I cannot just choose among them using R squared or using
RSS, because I'll always choose the biggest model.
And again, that just boils down to the fact that these
quantities are just based on the training error.
But I want a model with blow test error, because I want it
to do well on future observations that I haven't
seen, like patients who haven't walked into the clinic
yet for whom I'm going to want to predict
some sort of response.
And unfortunately, just choosing a model with the best
training error isn't going to give me a model that has a low
test error.
Because in general, training error is a really bad
predictor of test error, a really bad
estimate of test error.
And so RSS and R squared just are not suitable for the task
of choosing among models with different numbers of
predictors.